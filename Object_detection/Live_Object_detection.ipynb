{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d644988",
   "metadata": {},
   "source": [
    "# Model Prep.\n",
    "### There are two ways to get the model:\n",
    "\n",
    "1. Make it form scratch and don't froget to convert it to proto buffer (pb) fromat to use it in OpenCV as config file\n",
    "2. use a pre-trained one to work with.\n",
    "\n",
    "\n",
    "- I used Tensorflow pre-trained models and already prepared config pb from here: https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API you can find a tutorial of how to make a config pb file.\n",
    "- I choose Mobilenet v1 which is trained on cocco dataset you can find such info from here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md\n",
    "\n",
    "- you can find the file of cocco labels from here: https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b898472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item {\\n  name: \"/m/01g317\"\\n  id: 1\\n  display_name: \"person\"\\n}',\n",
       " ' {\\n  name: \"/m/0199g\"\\n  id: 2\\n  display_name: \"bicycle\"\\n}',\n",
       " ' {\\n  name: \"/m/0k4j\"\\n  id: 3\\n  display_name: \"car\"\\n}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('mscoco_label_map.pbtxt.txt')\n",
    "lines =file.read()\n",
    "lines.split('\\nitem')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749d3807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['null', 'person', 'bicycle']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare labels\n",
    "file = open('mscoco_label_map.pbtxt.txt')\n",
    "labels = [\"null\"] #because the first label with an id 1 in the label_map.pbtxt\n",
    "txt =file.read()\n",
    "line = txt.split('\\nitem')\n",
    "ind= [i.split('display_name: \"') for i in line]\n",
    "for i in range(len(ind)):\n",
    "    labels.append(ind[i][1].replace('\"\\n}',''))\n",
    "labels[:3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c490100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "model_path = 'ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb'\n",
    "config_file = 'ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt'\n",
    "model = cv2.dnn.readNetFromTensorflow(model_path,config_file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while (True):\n",
    "    success, frame = cap.read()\n",
    "    base_img = frame.copy()\n",
    "    original_size = frame.shape\n",
    "    tartget_size = (300,300)\n",
    "    img = cv2.resize(frame,tartget_size)\n",
    "    aspect_ratio_y = original_size[0] / tartget_size [0] #help us in to apply the work on base_img\n",
    "    aspect_ratio_x = original_size[1] / tartget_size[1]\n",
    "    imgblob= cv2.dnn.blobFromImage(img) #prepare img and returen 4-dim (1,3,300,300)\n",
    "    model.setInput(imgblob)\n",
    "    detections = model.forward()\n",
    "    detections_df = pd.DataFrame(detections[0][0], columns = [\"img_id\", \"label\", \"confidence\", \"left\", \"top\", \"right\", \"bottom\"])\n",
    "    detections_df = detections_df.query('confidence > 0.9') #label of 1 means face\\ 0:means background\n",
    "    for i,instance in detections_df.iterrows():\n",
    "        text = labels[int(detections_df['label'][i])] + str(round(detections_df['confidence'][i]*100)) + \"%\"\n",
    "        right = int(instance['right'] * 300) # multiply by 300 to see the results on the base_img remeber because we resize the img\n",
    "        left = int(instance['left'] * 300)\n",
    "        top = int(instance['top'] * 300)\n",
    "        bottom = int(instance ['bottom'] * 300)\n",
    "        detect_face = base_img[int(top *aspect_ratio_y): int(bottom*aspect_ratio_y), int(left*aspect_ratio_x): int(right*aspect_ratio_x)]\n",
    "        if detect_face.shape[0] > 0 and detect_face.shape[1] > 0:\n",
    "            cv2.putText(base_img, text , (int(left*aspect_ratio_x), int(top*aspect_ratio_y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 200), 2)\n",
    "            cv2.rectangle(base_img, (int(left*aspect_ratio_x), int(top*aspect_ratio_y)), (int(right*aspect_ratio_x), int(bottom*aspect_ratio_y)), (255, 0, 0), 4) #draw rectangle to main image\n",
    "    cv2.imshow('Object Detection', base_img) #show window of your camera\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #press q To exit\n",
    "        break\n",
    "    \n",
    "    \n",
    "cap.release()    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aabad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
